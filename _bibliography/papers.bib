---
---
@incollection{merchant2023object,
  title={Object measurement},
  author={Merchant, Fatima A and Shah, Shishir K and Castleman, Kenneth R},
  booktitle={Microscope Image Processing},
  pages={153--175},
  year={2023},
  publisher={Academic Press},
  preview={mip_book.jpg},
  abstract={Microscope Image Processing, Second Edition, introduces the basic fundamentals of image formation in microscopy including the importance of image digitization and display, which are key to quality visualization. Image processing and analysis are discussed in detail to provide readers with the tools necessary to improve the visual quality of images, and to extract quantitative information. Basic techniques such as image enhancement, filtering, segmentation, object measurement, and pattern recognition cover concepts integral to image processing. In addition, chapters on specific modern microscopy techniques such as fluorescence imaging, multispectral imaging, three-dimensional imaging and time-lapse imaging, introduce these key areas with emphasis on the differences among the various techniques.},
  html={https://shop.elsevier.com/books/microscope-image-processing/merchant/978-0-12-821049-9}
}

@article{rahman2024enhancing,
  selected={true},
  title={Enhancing lecture video navigation with AI generated summaries},
  author={Rahman, Mohammad Rajiur and Koka, Raga Shalini and Shah, Shishir K and Solorio, Thamar and Subhlok, Jaspal},
  journal={Education and Information Technologies},
  volume={29},
  number={6},
  pages={7361--7384},
  year={2024},
  publisher={Springer US New York},
  html={https://link.springer.com/article/10.1007/s10639-023-11866-7},
  abstract={Video is an increasingly important resource in higher education. A key limitation of lecture video is that it is fundamentally a sequential information stream. Quickly accessing the content aligned with specific learning objectives in a video recording of a classroom lecture is challenging. Recent research has enabled automatic reorganization of a lecture video into segments discussing different subtopics. This paper explores AI generation of visual and textual summaries of lecture video segments to improve navigation. A visual summary consists of a subset of images in the video segment that are considered the most unique and important by image analysis. A textual summary consists of a set of keywords selected from the screen text in the video segment by analyzing several factors including frequency, font size, time on screen, and existence in domain and language dictionaries. Evaluation was performed against keywords and summary images selected by human experts with the following results for the most relevant formulations. AI driven keyword selection yielded an F-1 score of 0.63 versus 0.26 for keywords sampled randomly from valid keyword candidates. AI driven visual summary yielded an F-1 score of 0.70 versus 0.59 for K-medoid clustering that is often employed for similar tasks. Surveys showed that 79% (72%) of the users agreed that a visual (textual) summary made a lecture video more useful. This framework is implemented in Videopoints, a real-world lecture video portal available to educational institutions.},
  doi={https://doi.org/10.1007/s10639-023-11866-7},
  preview={lecturevideo-2024.jpg}
}

@inproceedings{nguyen2024temporal,
  title={Temporal 3d shape modeling for video-based cloth-changing person re-identification},
  author={Nguyen, Vuong D and Mantini, Pranav and Shah, Shishir K},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={173--182},
  year={2024}
}

@inproceedings{khaldi2024unsupervised,
  title={Unsupervised person re-identification in aerial imagery},
  author={Khaldi, Khadija and Nguyen, Vuong D and Mantini, Pranav and Shah, Shishir},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={260--269},
  year={2024}
}

@inproceedings{nguyen2024contrastive,
  title={Contrastive viewpoint-aware shape learning for long-term person re-identification},
  author={Nguyen, Vuong D and Khaldi, Khadija and Nguyen, Dung and Mantini, Pranav and Shah, Shishir},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1041--1049},
  year={2024},
  html={https://openaccess.thecvf.com/content/WACV2024/html/Nguyen_Contrastive_Viewpoint-Aware_Shape_Learning_for_Long-Term_Person_Re-Identification_WACV_2024_paper.html},
  abstract={Traditional approaches for Person Re-identification (Re-ID) rely heavily on modeling the appearance of persons. This measure is unreliable over longer durations due to the possibility for changes in clothing or biometric information. Furthermore, viewpoint changes significantly degrade the matching ability of these methods. In this paper, we propose "Contrastive Viewpoint-aware Shape Learning for Long-term Person Re-Identification" (CVSL) to address these challenges. Our method robustly extracts local and global texture-invariant human body shape cues from 2D pose using the Relational Shape Embedding branch, which consists of a pose estimator and a shape encoder built on a Graph Attention Network. To enhance the discriminability of the shape and appearance of identities under viewpoint variations, we propose Contrastive Viewpoint-aware Losses (CVL). CVL leverages contrastive learning to simultaneously minimize the intra-class gap under different viewpoints and maximize the inter-class gap under the same viewpoint. Extensive experiments demonstrate that our proposed framework outperforms state-of-the-art methods on long-term person Re-ID benchmarks.},
  preview={cvsl-2024.png},
  selected={true}
}

@inproceedings{biswas2023identification,
  title={Identification of Visual Objects in Lecture Videos with Color and Keypoints Analysis},
  author={Biswas, Dipayan and Shah, Shishir and Subhlok, Jaspal},
  booktitle={2023 IEEE International Symposium on Multimedia (ISM)},
  pages={315--320},
  year={2023},
  organization={IEEE}
}

@article{nguyen2024attention,
  selected={false},
  title={Attention-based shape and gait representations learning for video-based cloth-changing person re-identification},
  author={Nguyen, Vuong D and Mirza, Samiha and Mantini, Pranav and Shah, Shishir K},
  journal={arXiv preprint arXiv:2402.03716},
  year={2024}
}

@article{mirza2024data,
  selected={false},
  title={Data quality aware approaches for addressing model drift of semantic segmentation models},
  author={Mirza, Samiha and Nguyen, Vuong D and Mantini, Pranav and Shah, Shishir K},
  journal={arXiv preprint arXiv:2402.07258},
  year={2024}
}

@article{nguyen2024ccpa,
  selectec={false},
  title={Ccpa: Long-term person re-identification via contrastive clothing and pose augmentation},
  author={Nguyen, Vuong D and Shah, Shishir K},
  journal={arXiv preprint arXiv:2402.14454},
  year={2024}
}

@inproceedings{nguyen2024contrastive,
  title={Contrastive clothing and pose generation for cloth-changing person re-identification},
  author={Nguyen, Vuong D and Mantini, Pranav and Shah, Shishir K},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7541--7549},
  year={2024}
}

@inproceedings{nguyen2024tackling,
  title={Tackling domain shifts in person re-identification: A survey and analysis},
  author={Nguyen, Vuong D and Mirza, Samiha and Zakeri, Abdollah and Gupta, Ayush and Khaldi, Khadija and Aloui, Rahma and Mantini, Pranav and Shah, Shishir K and Merchant, Fatima},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4149--4159},
  year={2024}
}

@inproceedings{nguyen2024occluded,
  title={Occluded cloth-changing person re-identification via occlusion-aware appearance and shape reasoning},
  author={Nguyen, Vuong D and Mantini, Pranav and Shah, Shishir K},
  booktitle={2024 IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)},
  pages={1--8},
  year={2024},
  organization={IEEE}
}

@inproceedings{nguyen2024acml,
  title={ACML: Attention-based cross-modality learning for cloth-changing and occluded person re-identification},
  author={Nguyen, Vuong D and Mantini, Pranav and Shah, Shishir K},
  booktitle={2024 IEEE International Conference on Image Processing (ICIP)},
  pages={2396--2402},
  year={2024},
  organization={IEEE},
  html={https://ieeexplore.ieee.org/abstract/document/10647794},
  doi={https://doi.org/10.1109/ICIP51287.2024.10647794},
  abstract={Person Re-Identification (Re-ID) aims at matching a person captured by a non-overlapping camera system. Real-world Re-ID presents challenges like clothing changes and occlusions, which limits the applicability of traditional appearance-based methods. Cloth-Changing Re-ID (CCRe-ID) methods that rely on cloth-invariant modalities, such as shape, gait, etc., ignore occlusions and fail to mine the complementary relationship across modalities. Meanwhile, methods that explicitly focus on occlusion management struggle with cloth-changing scenarios. To address these, we propose ACML: Attention-based Cross-Modality Learning, the first framework to tackle both clothing changes and occlusion in Re-ID. Our lightweight framework comprises a unified network with cascaded Cross-Attention Blocks that extracts appearance and shape features collaboratively, enhancing robustness under clothing changes, viewpoint variations, and poor illumination conditions. Inputs to the network are produced by our novel occlusion synthesis module, which not only helps exposing the model to occlusions but also guides the model to adaptively attend to informative cues and reduce noise. Experiments demonstrate the effectiveness of ACML on both CCRe-ID and occluded Re-ID datasets.},
  preview={acml-2024.png},
  selected={true}
}

@inproceedings{nguyen2024occlusion,
  title={Occlusion-aware Cross-Attention Fusion for Video-based Occluded Cloth-Changing Person Re-Identification},
  author={Nguyen, Vuong D and Mantini, Pranav and Shah, Shishir K},
  booktitle={2024 IEEE International Joint Conference on Biometrics (IJCB)},
  pages={1--11},
  year={2024},
  organization={IEEE},
  html={https://ieeexplore.ieee.org/abstract/document/10744425},
  abstract={Video-based Person Re-Identification (Re-ID) is an important task in video surveillance analysis. Real-world video-based Re-ID commonly suffers from clothing changes and occlusions, which severely degenerates performance of traditional Re-ID methods. In this paper, we introduce a challenging yet practical task called Video-based Occluded Cloth-Changing Re-ID (VOCCRe-ID). To tackle occlusions, we propose an occlusion synthesis strategy to expose the model to real-world occlusion variations. To mitigate unreliable appearance caused by clothing changes, we couple body shape information from the normalized silhouette sequence. Then, we propose a cross-attention fusion mechanism to capture the complementary relationships between appearance and shape under occlusions, thus enhancing Re-ID robustness. In addition, since there are no dataset for VOCCRe-ID, we build the large-scale Occluded-VCCR dataset which explicitly presents occlusions and contains the most clothing variations. Extensive experiments show that we achieve SOTA performance over previous methods.},
  doi={https://doi.org/1010.1109/IJCB62174.2024.10744425}
}

@inproceedings{mirza2024recall,
  selected={true},
  title={Recall-Based Knowledge Distillation for Data Distribution Based Catastrophic Forgetting in Semantic Segmentation},
  author={Mirza, Samiha and Gala, Apurva and Devarakota, Pandu and Nguyen, Vuong D and Mantini, Pranav and Shah, Shishir K},
  booktitle={International Conference on Pattern Recognition},
  pages={98--113},
  year={2024},
  organization={Springer Nature Switzerland Cham},
  html={https://link.springer.com/chapter/10.1007/978-3-031-78347-0_7},
  doi={https://doi.org/10.1007/978-3-031-78347-0_7},
  abstract={Semantic segmentation involves labeling each pixel in an image with a corresponding class label, enabling detailed scene understanding. In dynamic environments, where conditions change over time, incremental learning techniques are essential for updating segmentation models with newly acquired data. However, incremental segmentation faces the challenge of catastrophic forgetting, where models lose previously learned knowledge when trained on new data distributions. To address this, we propose a recall-based knowledge distillation approach for stable segmentation model training across dynamic environments. Our method combines the strengths of knowledge distillation and recall learning to enhance the model’s ability to recall information from previous data distributions while adapting to new ones. By reintroducing a small portion of the previous dataset during training and applying tailored distillation techniques, our approach mitigates catastrophic forgetting and improves the robustness of these models. Through comprehensive evaluations, we demonstrate the effectiveness of our approach in two scenarios: salt segmentation in seismic datasets and tumor segmentation in MRI datasets. Our method offers a promising solution for addressing the challenges of catastrophic forgetting in incremental semantic segmentation, facilitating the development of more adaptive and reliable computer vision systems in dynamic environments.},
  preview={recall-distillation-2024.jpg}
}

@inproceedings{nguyen2024crossvit,
  title={CrossViT-ReID: Cross-Attention Vision Transformer for Occluded Cloth-Changing Person Re-Identification},
  author={Nguyen, Vuong D and Mantini, Pranav and Shah, Shishir K},
  booktitle={Proceedings of the Asian Conference on Computer Vision},
  pages={3982--3999},
  year={2024}
}

@inproceedings{nguyen2024cross,
  title={Cross-Modality Complementary Learning for Video-Based Cloth-Changing Person Re-identification},
  author={Nguyen, Vuong D and Mantini, Pranav and Shah, Shishir K},
  booktitle={Proceedings of the Asian Conference on Computer Vision},
  pages={88--107},
  year={2024}
}

@article{nguyen2025occlusion,
  selected={true},
  title={Occlusion-aware appearance and shape learning for occluded cloth-changing person re-identification},
  author={Nguyen, Vuong D and Mantini, Pranav and Shah, Shishir K},
  journal={Pattern Analysis and Applications},
  volume={28},
  number={2},
  pages={1--17},
  year={2025},
  publisher={Springer London},
  preview={occ-cc-2025.jpg},
  doi={https://doi.org/10.1007/s10044-025-01459-0},
  html={https://link.springer.com/article/10.1007/s10044-025-01459-0},
  abstract={In recent years, Person Re-Identification (Re-ID) has seen remarkable progress in addressing the issue of clothing changes. However, in real-world scenarios, Re-ID is often further challenged by occlusions, while very little research has been conducted to explicitly tackle these two challenges simultaneously. To this end, we propose a method for Occluded Cloth-Changing Person Re-ID (OCCRe-ID) termed “OASL: Occlusion-aware Appearance and Shape Learning”. OASL introduces a plug-and-play occlusion handling strategy which can be seamlessly integrated into existing Re-ID methods, enabling them to reason discriminative appearance and shape features under occlusions. Specifically, our approach leverages occlusion type information to achieve two key objectives for occlusion-awareness: (1) guide the backbone to focus on extracting identity-aware appearance features from non-occluded image regions and reason features from occluded ones, and (2) recover pose keypoints from occluded regions for mitigating occlusions in shape encoding. Additionally, we construct E-PRCC, the first dataset for OCCRe-ID, with the aim of facilitating further research in this practical domain. Extensive experiments conducted on E-PRCC, LTCC, Occluded-REID, DeepChange, and Market-1501 datasets demonstrate that OASL achieves state-of-the-art performance, offering a robust solution to the dual challenges of occlusions and clothing changes in Person Re-ID.}
}

