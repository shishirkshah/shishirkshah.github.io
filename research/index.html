<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> research | Shishir K. Shah </title> <meta name="author" content="Shishir K. Shah"> <meta name="description" content="Research areas and projects in the Quantitative Imaging Lab"> <meta name="keywords" content="computer-vision, machine learning, image-understanding, pattern-recognition, quantitative-microscopy"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%AC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shishirkshah.github.io/research/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Shishir K. Shah </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/research/">research <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <h1 id="human-motion-and-behavior-analysis">Human Motion and Behavior Analysis</h1> <p><img src="/assets/img/research/5floor.png" alt="Human Motion Analysis" class="research-image-float"></p> <p>Human behavior has been extensively studied by sociologists to understand social interactions and crowd dynamics. It has been argued that characteristics that dictate human motion constitute a complex interplay between human physical, environmental, and psychosocial characteristics. It is a common observation that people, whenever free to move about in an environment, tend to respect certain patterns of movement. More often, these patterns of movement are dominated by social mechanisms.</p> <p>While much of computer vision has focused on studies that try to model the physical and environmental characteristics, psychosocial influences have largely been overlooked. Broadly speaking, human motion attributed to psychological and sociological characteristics may be evaluated at three distinct levels; individual, interaction among individuals, and group dynamics.</p> <p>Studies of collective locomotion have shown that social interactions play an important role in structuring human behavior, which in turn influences local human motion. Humans when together in large gatherings orient their actions to each other and move in concert with each other. In contrast, individuality of a human and the notion of accounting for independent decision making ability leads to possibilities of emergent behaviors.</p> <p>Human movements are generally driven by purpose, making decisions locally based on the extent of information available and the cognitive capacity for calculation, prediction, and action. Each of these distinct observations have led to formalisms that either try to model human motion dynamics as a flow with fluid-like properties or consider human motion to be individualistic with local interactions leading to complex dynamics.</p> <p>The prior formalism assumes the motion of individuals to follow the overall dynamics in an environment to maintain continuum while the latter focuses on self-organizing effects. In either case, our ability to increase our understanding of human motion and activity can be extended through the development of models and algorithms that integrate social cues that codify human behavior.</p> <p>Our interests and research efforts in this area have led to contributions of algorithms that propose models to code human-human and human-environment interactions through fundamentals of proxemics. These have been successfully applied to problems in human tracking, detection of groups of people, human group activity recognition, and leveraging of group structures for re-identification of humans across distributed camera networks.</p> <p>This research has been and continues to be supported in part by the National Institute of Standards and Technology, US Army Research Labs and the Department of Justice.</p> <p><img src="/assets/img/research/network_structure.png" alt="Network Structure" class="research-img"></p> <h1 id="wide-area-surveillance">Wide-Area Surveillance</h1> <p><img src="/assets/img/research/frame418.png" alt="Wide-Area Surveillance" class="research-image-float"></p> <p>This effort is related to the problem of target acquisition, tracking, and recognition in a sparse sensor network. While multiple cameras with overlapping field of view can enhance the capability and performance of video surveillance applications, providing fault-tolerance and robustness for issues such as object occlusion by achieving overlapping field of views across a large multi-camera network is not always feasible.</p> <p>This requires coordination and synchronization of cameras within the network, which is not dependent on the target object. In addressing these challenges, our research goals have been to develop an intelligent, non-obtrusive, real-time, continuous monitoring system for assessing activity and predicting emergent suspicious and criminal behavior across a network of distributed cameras.</p> <p>The envisioned system consists of two main modules, namely:</p> <ol> <li>A non-obtrusive tracking system that can continuously: <ul> <li>Track all objects across a network of distributed cameras</li> <li>Analyze the spatio-temporal movement pattern of each object</li> <li>Detect and measure descriptive information continuously of each tracked object</li> </ul> </li> <li>A decision system that can: <ul> <li>Correlate each object's spatio-temporal patterns with others and generate models of suspicious/criminal activity</li> <li>Generate activity alerts for security personnel who monitor and make critical decisions</li> </ul> </li> </ol> <p>To that extent, over the past 5 years, we have successfully:</p> <ul> <li>Functionalized a state-of-the-art distributed surveillance camera network with over 25 cameras (indoors and outdoors) for data collection and validation studies</li> <li>Developed multi-human tracking algorithms capable of robust object label management and trajectory generation</li> <li>Developed algorithms for human reacquisition (re-identification) across non-overlapping cameras</li> <li>Developed models and algorithms for activity analysis</li> <li>More recently, developed algorithms for recovering (predicting) motion trajectories of tracked objects across non-overlapping cameras</li> </ul> <p>This research has been and continues to be supported in part by the US Army Research Labs, The Texas Advanced Research Program, National Science Foundation, and the Department of Justice.</p> <p><img src="/assets/img/research/log_165.png" alt="Surveillance Log" class="research-img"></p> <h1 id="facial-analysis">Facial Analysis</h1> <p><img src="/assets/img/research/Views.png" alt="Facial Analysis" class="research-image-float"></p> <p>Face recognition is one of the most attractive biometric since it can be done passively and unobtrusively at a comfortable distance. In addressing the challenges across the multitude of applications, many face recognition systems have been developed over the past few years.</p> <p>Most facial matching systems rely on two-dimensional (2D) facial images and their performance is broadly affected by varying illumination conditions, change in pose, and poor image quality (e.g., blurry images, low resolution). Systems leveraging strictly 2D facial image analysis have high false accept/reject rates, suboptimal response time scaling with database size, and strict pose/illumination acquisition requirements.</p> <p>On the other hand, three-dimensional (3D) face recognition does not suffer from inherent problems of pose and illumination variations. Nonetheless, it is not feasible to limit such systems to only 3D data. Thus, there is a technology gap between these two types of systems (2D vs. 3D).</p> <p>Our research interests and goals have been to bridge this gap by specifically addressing facial matching under availability of 2D images. The driving hypothesis has been that through the use of 3D model of the face and by addressing variations related to pose, illumination, and resolution, it is possible to improve face matching and in general facial analysis.</p> <p>To that extent, much of our research contributions have been related to developing methods and systems for:</p> <ul> <li>Face recognition</li> <li>Facial image super-resolution</li> <li>Illumination modeling and correction</li> <li>Facial landmark detection</li> <li>Robust facial signature extraction</li> </ul> <p>This research has been and continues to be supported in part by the US Army Research Labs, and the Intelligence Advanced Research Projects Activity.</p> <style>body{font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Helvetica,Arial,sans-serif;line-height:1.6;color:#333;max-width:900px;margin:0 auto;padding:0 1em}h1{font-size:2em;margin-top:1.5em;margin-bottom:1em;color:#2a5885;clear:both}h2{font-size:1.5em;margin-top:1.5em;margin-bottom:1em;color:#2a5885}p{margin-bottom:1em}ul,ol{margin-bottom:1em;padding-left:2em}li{margin-bottom:.5em}.research-image-float{float:right;width:300px;margin:0 0 1em 2em;border:10px solid white;box-shadow:0 2px 4px rgba(0,0,0,0.1)}.research-img{display:block;max-width:100%;height:auto;margin:2em auto;border:10px solid white;box-shadow:0 2px 4px rgba(0,0,0,0.1);clear:both}@media(max-width:768px){.research-image-float{float:none;width:100%;max-width:400px;margin:1em auto}}</style> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Shishir K. Shah. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>